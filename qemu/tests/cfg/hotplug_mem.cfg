# Notes:
#    Before start testing, please ensure your host OS support hugepage,
#    and ensure you host memory more than 6GB.
#    And memory hotplug need guest OS support, so please ensure your
#    guest OS really supported it.
- hotplug_memory:
    type = hotplug_mem
    mem_fixed = 1024
    slots_mem = 4
    size_mem = 1G
    maxmem_mem = 32G
    mem_devs = mem1
    login_timeout = 600
    no Host_RHEL.6
    no RHEL.5 RHEL.6
    no Windows..i386
    no WinXP Win2000 Win2003 WinVista
    # Notes:
    #    The threshold on ppc was confirmed with developer
    #    since crashkernel size is much bigger than x86
    ppc64,ppc64le:
        threshold = 0.15
    sub_test_wait_time = 0
    variants numa_nodes:
        - one:
            guest_numa_nodes = "node0"
            del numa_mem
            del numa_cpus
            del numa_nodeid
            only after
            only guest_reboot
        - two:
            guest_numa_nodes = "node0 node1"
            del numa_mem
            del numa_cpus
            numa_nodeid = 0
            memdevs += " mem2"
            node_dimm_mem2 = 0
            node_dimm_mem1 = 1
            numa_nodeid_node0 = 0
            numa_nodeid_node1 = 1
    variants:
        - policy_default:
            policy_mem = default
        - policy_bind:
            policy_mem = bind
            host-nodes = 0
        - policy_interleave:
            policy_mem = interleave
            host-nodes = 0
            only guest_reboot
            only after
        - policy_preferred:
            policy_mem = preferred
            host-nodes = 0
            only guest_reboot
            only after
        - no_policy:
            del policy_mem
            only guest_reboot
            only after
    variants:
        - backend_ram:
            backend_mem = memory-backend-ram
        - backend_file:
            # Notes:
            #     Before start testing, please ensure your host
            # kernel has support hugpage and have enough memory
            # to create guest memory
            backend_mem = memory-backend-file
            setup_hugepages = yes
            # mem path should be the hugpage path
            mem-path = /mnt/kvm_hugepage
            pre_command = "echo 3 > /proc/sys/vm/drop_caches"
            pre_command_noncritical = yes
            variants:
                - 2M_hugepage:
                    # default pagesize is 2M, 2G guest memory need to allocate
                    # in hugepage, so page nubmer is:
                    #    target_hugepages = (size_mem / page_size) * 2 + 10
                    target_hugepages = 2058
                    expected_hugepage_size = 2048
                - 16M_hugepage:
                    only pseries
                    target_hugepages = 266
                    expected_hugepage_size = 16384
                - 1G_hugepage:
                    # Warning:
                    #     Please don't forget to update host kernel line to enable
                    # 1G hugepage support before lanuch this test, else test will
                    # failed.
                    target_hugepages = 5
                    expected_hugepage_size = 1048576
                    only one
                    only guest_reboot
                    only no_policy
                    no pseries
    variants operation:
        - unplug:
            no Windows
            variants:
                - buildin_memory:
                    mem_devs += " buildin"
                    target_mem = "buildin"
                - pluged_memory:
                    plug_mem = "plug"
                    target_mem = "plug"
                - unused_memory:
                    target_mem = "unused"
                    mem_devs += " ${target_mem}"
                    use_mem_unused = no
        - hotplug:
            target_mem = "plug"
    variants sub_test:
        - vm_system_reset:
            sub_type = boot
            reboot_method = system_reset
            sleep_before_reset = 0
        - guest_reboot:
            sub_test_wait_time = 2
            sub_type = boot
            reboot_method = shell
            kill_vm_on_error = yes
            reboot_count = 1
        - guest_migration:
            sub_type = migration
            Linux:
                migration_test_command = help
                migration_bg_command = "cd /tmp; nohup tcpdump -q -i any -t ip host localhost"
                migration_bg_check_command = pgrep tcpdump
                migration_bg_kill_command = pkill -9 tcpdump
            Windows:
                migration_test_command = ver && vol
                migration_bg_command = start ping -t localhost
                migration_bg_check_command = tasklist | find /I "ping.exe"
                migration_bg_kill_command = taskkill /IM ping.exe /F
            kill_vm_on_error = yes
            iterations = 2
            used_mem = 1024
            mig_timeout = 3600
            ping_pong = 1
            migration_protocol = "tcp"
            only after
        - pause_vm:
            sub_type = stop_continue
            pause_time = 300
            wait_resume_timeout = "${pause_time}"
            sub_test_wait_time = 60
        - stress:
            sub_test_wait_time = 60
            Windows:
                timeout = 600
                autostress = yes
                sub_type = win_heavyload
                backup_image_before_testing = yes
                restore_image_after_testing = yes
                install_path = "C:\Program Files (x86)\JAM Software\HeavyLoad"
                config_cmd = 'setx -m path "%PATH%;${install_path};"'
                install_cmd = "start /wait DRIVE:\HeavyLoadSetup.exe /verysilent"
                x86_64:
                    install_path = "C:\Program Files (x86)\JAM Software\HeavyLoad"
                i386, i686:
                    install_path = "C:\Program Files\JAM Software\HeavyLoad"
            Linux:
                sub_type = linux_stress
                test_timeout = 1800
    variants stage:
        - before:
        - after:
        - during:
