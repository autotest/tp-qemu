- hotplug_mem_migration:
    type = hotplug_mem_migration
    mem_fixed = 8192
    smp = 4
    slots_mem = 4
    size_mem = 2G
    maxmem_mem = 32G
    login_timeout = 600
    pre_command = "sync && echo 3 > /proc/sys/vm/drop_caches"
    no Host_RHEL.6 Host_RHEL.7 Host_RHEL.8
    no RHEL.6 RHEL.7 RHEL.8
    no Windows
    threshold = 0.12
    # Notes:
    #    The threshold on arm and ppc was confirmed with developer
    #    since crashkernel size is much bigger than x86
    aarch64,ppc64,ppc64le:
        threshold = 0.15
    max_vms = 2
    migration_test_command = help
    migration_bg_command = "cd /tmp; nohup tcpdump -q -i any -t ip host localhost"
    migration_bg_check_command = pgrep tcpdump
    migration_bg_kill_command = pkill -9 tcpdump
    kill_vm_on_error = yes
    iterations = 2
    mig_timeout = 1200
    ping_pong = 1
    migration_protocol = "tcp"
    target_mems = "plug1 plug2"
    cmd_check_online_mem = lsmem
    cmd_new_folder = ' rm -rf /tmp/numa_test/ && mkdir /tmp/numa_test'
    numa_test = 'numactl -m %s dd if=/dev/urandom of=/tmp/numa_test/test '
    numa_test += 'bs=1k count=%d && rm -rf /tmp/numa_test/'
    stress_args = '--cpu 4 --io 4 --vm 2 --vm-bytes 4096M'
    variants with_cache:
        - @default:
        - enable_dirty_ring:
            only i386, x86_64, aarch64
            required_qemu = [6.3.0,)
            aarch64:
                required_qemu = [8.1.0,)
            only with_multi_numa_nodes
            extra_params += "-accel kvm,dirty-ring-size=65536"
            disable_kvm = yes
            enable_kvm = no
    variants numa_nodes:
        - with_single_numa_node:
            guest_numa_nodes = "node0"
            mem_devs = "memN0"
            use_mem_memN0 = "no"
            size_mem_memN0 = 8192M
            backend_mem_memN0 = memory-backend-ram
            numa_memdev_node0 = mem-memN0
            del numa_mem
            del numa_cpus
            del numa_nodeid
        - with_multi_numa_nodes:
            guest_numa_nodes = "node0 node1"
            del numa_mem
            del numa_cpus
            numa_nodeid = 0
            mem_devs = "memN0 memN1"
            numa_memdev_node0 = mem-memN0
            numa_memdev_node1 = mem-memN1
            use_mem_memN0 = "no"
            use_mem_memN1 = "no"
            size_mem_memN0 = 4096M
            size_mem_memN1 = 4096M
            backend_mem_memN0 = memory-backend-ram
            backend_mem_memN1 = memory-backend-ram
            node_dimm_mem2 = 0
            node_dimm_mem1 = 1
            numa_nodeid_node0 = 0
            numa_nodeid_node1 = 1
            node_dimm_plug1 = 0
            node_dimm_plug2 = 1
            # Due to known issue
            RHEL.9.0.0:
                aarch64:
                    numa_cpus_node0 = "0,1"
                    numa_cpus_node1 = "2,3"
    variants:
        - backend_ram:
            backend_mem = memory-backend-ram
            del discard-data
        - backend_file:
            # Notes:
            #     Before start testing, please ensure your host
            # kernel has support hugpage and have enough memory
            # to create guest memory
            backend_mem = memory-backend-file
            setup_hugepages = yes
            # mem path should be the hugpage path
            mem-path = /mnt/kvm_hugepage
            pre_command = "echo 3 > /proc/sys/vm/drop_caches"
            pre_command_noncritical = yes
        - backend_memfd:
            no Host_RHEL.m7 Host_RHEL.m8.u0 Host_RHEL.m8.u1 Host_RHEL.m8.u2
            backend_mem = memory-backend-memfd
            setup_hugepages = yes
            hugetlb_mem = yes
            only with_multi_numa_nodes
